{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d986335",
   "metadata": {},
   "source": [
    "# Yahoo Finance Webscraping Project\n",
    "\n",
    "Author: Muhammad Fouzan Akhter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f891d5",
   "metadata": {},
   "source": [
    "The code for a web scraping project that targets Yahoo Finance is shown below. Underscoring the importance of following website privacy policies is crucial for any online scraping project. It is imperative to highlight that this project is scraping entirely publicly accessible data from Yahoo Finance while adhering to the platform's privacy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19440e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing required packages:\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries:\n",
    "from google.colab import files\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa0860",
   "metadata": {},
   "source": [
    "**This Project is coded in the Google Colab Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5ba5b",
   "metadata": {},
   "source": [
    "To navigate down the Yahoo Finance page, press and hold the space key on your keyboard. Continue holding it until you reach the maximum scroll. Once you've reached the bottom, download the webpage as an HTML file. Finally, upload the downloaded file to the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_files = files.upload()\n",
    "html_filename = list(uploaded_files.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7553678",
   "metadata": {},
   "source": [
    "The provided loop extracts data from the HTML file, including the title, author, datetime, readtime, content, tags, and link of each article. Ensure an active internet connection before executing the code. A tqdm loadbar is incorporated to enhance time management during code execution. All the extracted data will be stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "if html_filename:\n",
    "    html_content = uploaded_files[html_filename].decode('utf-8')\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    linkslist = set()\n",
    "\n",
    "    for link in tqdm(soup.find_all(\"a\", class_=\"js-content-viewer\"), desc=\"Collecting Links\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href:\n",
    "            linkslist.add(href)\n",
    "\n",
    "    print(\"Extraction of Links Completed\")\n",
    "    print(f\"{len(linkslist)} Unique Links Extracted\")\n",
    "    print(\"********************\")\n",
    "    data = []\n",
    "\n",
    "    for link in tqdm(linkslist, desc=\"Article Scraper Running\"):\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            article_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title_element = article_soup.find('h1', attrs={'data-test-locator': 'headline'})\n",
    "            extracted_title = title_element.text.strip() if title_element else \"Title not found\"\n",
    "            author_element = article_soup.find('span', class_='caas-author-byline-collapse')\n",
    "            author_name_element = author_element.find('a', class_='link') if author_element else None\n",
    "            author_name = author_name_element.text.strip() if author_name_element else None\n",
    "            if author_name is None:\n",
    "                author_name_element = author_element.find_all('span')\n",
    "                if author_name_element:\n",
    "                    author_name = author_name_element[0].text.strip()\n",
    "            if author_name is None:\n",
    "                author_text = str(author_element)\n",
    "                author_match = re.search(r'<span class=\"caas-author-byline-collapse\">(.*?)<\\/span>', author_text)\n",
    "                if author_match:\n",
    "                    author_name = author_match.group(1).strip()\n",
    "                else:\n",
    "                    author_name = \"Author not found\"\n",
    "            datetime_element = article_soup.find('time', class_='caas-attr-meta-time')\n",
    "            datetime_value = datetime_element.get_text(strip=True) if datetime_element else \"Date and Time not found\"\n",
    "            readtime_element = article_soup.find('span', class_='caas-attr-mins-read')\n",
    "            readtime = readtime_element.text.strip() if readtime_element else \"Read time not found\"\n",
    "            content_elements = article_soup.find_all('div', class_='caas-body')\n",
    "            content = '\\n'.join([p.get_text() for p in content_elements])\n",
    "            json_ld_script = article_soup.find('script', {'type': 'application/ld+json'})\n",
    "            if json_ld_script:\n",
    "                json_data = json.loads(json_ld_script.string)\n",
    "                tags = json_data.get('keywords', [])\n",
    "            else:\n",
    "                tags = []\n",
    "            data.append({\n",
    "                'Title': extracted_title,\n",
    "                'Author': author_name,\n",
    "                'Datetime': datetime_value,\n",
    "                'ReadTime': readtime,\n",
    "                'Content': content,\n",
    "                'Tags': tags,\n",
    "                'Link': link\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "else:\n",
    "    print(\"The uploaded HTML file was not found. Please recheck the name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313f06d",
   "metadata": {},
   "source": [
    "**Viewing Scraped Data in Python Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying scraped data in python environment:\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5cc9d",
   "metadata": {},
   "source": [
    "**Downloading Scraped Data as a CSV File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14028c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading scraped data as CSV:\n",
    "df.to_csv('my_data.csv', index=False)\n",
    "files.download('my_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055bb9e",
   "metadata": {},
   "source": [
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
